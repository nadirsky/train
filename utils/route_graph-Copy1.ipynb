{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction of Route Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************************************\n",
    "This notebook walks you through the code in the [route_graph.py](https://gitlab.crowdai.org/SBB/train-schedule-optimisation-challenge-starter-kit/blob/master/utils/route_graph.py) script. It contains code to build directed graphs in the [networkx](https://networkx.github.io/) package from the `routes` in the problem instances. \n",
    "\n",
    "This should help you to better understand the `routes` and how to work with them. It may also prove useful in your solving algorithm, such as for finding zero-penalty paths or the like. If you don't like to work with the networkx package, you can copy the logic from the functions `from_node_id` and `to_node_id`, which assign the node id's, to the graph library of your choice.\n",
    "\n",
    "To run the following code, please ensure that you use Pyhton >= 3.6 and first install the following Python libraries:\n",
    "- networkx\n",
    "- matplotlib\n",
    "**************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from functools import partial\n",
    "from itertools import chain, product, starmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns \"from\"-node id for a given `route_section`. The crucial point is that nodes with common `route_alternative_marker`s are identified as the same node in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_node_id(route_path, route_section, index_in_path):\n",
    "    if \"route_alternative_marker_at_entry\" in route_section.keys() and \\\n",
    "            route_section[\"route_alternative_marker_at_entry\"] is not None and \\\n",
    "            len(route_section[\"route_alternative_marker_at_entry\"]) > 0:\n",
    "                return \"(\" + str(route_section[\"route_alternative_marker_at_entry\"][0]) + \")\"\n",
    "    else:\n",
    "        if index_in_path == 0:  # can only get here if this node is a very beginning of a route\n",
    "            return \"(\" + str(route_section[\"sequence_number\"]) + \"_beginning)\"\n",
    "        else:\n",
    "            return \"(\" + (str(route_path[\"route_sections\"][index_in_path - 1][\"sequence_number\"]) + \"->\" +\n",
    "                          str(route_section[\"sequence_number\"])) + \")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns \"to\"-node id for a given `route_section`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_node_id(route_path, route_section, index_in_path):\n",
    "    if \"route_alternative_marker_at_exit\" in route_section.keys() and \\\n",
    "            route_section[\"route_alternative_marker_at_exit\"] is not None and \\\n",
    "            len(route_section[\"route_alternative_marker_at_exit\"]) > 0:\n",
    "\n",
    "                return \"(\" + str(route_section[\"route_alternative_marker_at_exit\"][0]) + \")\"\n",
    "    else:\n",
    "        if index_in_path == (len(route_path[\"route_sections\"]) - 1): # meaning this node is a very end of a route\n",
    "            return \"(\" + str(route_section[\"sequence_number\"]) + \"_end\" + \")\"\n",
    "        else:\n",
    "            return \"(\" + (str(route_section[\"sequence_number\"]) + \"->\" +\n",
    "                          str(route_path[\"route_sections\"][index_in_path + 1][\"sequence_number\"])) + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_to_leaf_paths(G):\n",
    "    \"\"\"Yields root-to-leaf paths in a directed acyclic graph.\n",
    "\n",
    "    `G` must be a directed acyclic graph. If not, the behavior of this\n",
    "    function is undefined. A \"root\" in this graph is a node of in-degree\n",
    "    zero and a \"leaf\" a node of out-degree zero.\n",
    "\n",
    "    When invoked, this function iterates over each path from any root to\n",
    "    any leaf. A path is a list of nodes.\n",
    "\n",
    "    \"\"\"\n",
    "    roots = (v for v, d in G.in_degree() if d == 0)\n",
    "    leaves = (v for v, d in G.out_degree() if d == 0)\n",
    "    all_paths = partial(nx.all_simple_paths, G)\n",
    "    # TODO In Python 3, this would be better as `yield from ...`.\n",
    "    chaini = chain.from_iterable\n",
    "    return chaini(starmap(all_paths, product(roots, leaves)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "We build the route graphs for the two routes in the Challenge sample_instance.\n",
    "\n",
    "For large graphs, you probably want to deactivate the printint output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = \"../sample_files/sample_scenario.json\"  # adjust path to the sample instance if it is not located there\n",
    "# data = \"../problem_instances/01_dummy.json\"\n",
    "data = \"../problem_instances/02_a_little_less_dummy.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>release_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZUE_A3-A</td>\n",
       "      <td>00:00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZUE_A3-B</td>\n",
       "      <td>00:00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZUE_A4-A</td>\n",
       "      <td>00:00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZUE_A4-B</td>\n",
       "      <td>00:00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZUE_A5-A</td>\n",
       "      <td>00:00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id release_time\n",
       "0  ZUE_A3-A     00:00:10\n",
       "1  ZUE_A3-B     00:00:10\n",
       "2  ZUE_A4-A     00:00:10\n",
       "3  ZUE_A4-B     00:00:10\n",
       "4  ZUE_A5-A     00:00:10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(data) as d:\n",
    "    scenario = json.load(d)\n",
    "    \n",
    "resources = pd.DataFrame.from_dict(scenario['resources'])\n",
    "resources['release_time'] = pd.to_timedelta(resources['release_time'].apply(lambda x: x.split('PT')[-1]))\n",
    "resources = resources.drop('following_allowed', axis=1)\n",
    "resources.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>route</th>\n",
       "      <th>section_requirements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2408</td>\n",
       "      <td>2408</td>\n",
       "      <td>[{'sequence_number': 1, 'section_marker': 'ZGP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>856</td>\n",
       "      <td>856</td>\n",
       "      <td>[{'sequence_number': 1, 'section_marker': 'ZGP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2620</td>\n",
       "      <td>2620</td>\n",
       "      <td>[{'sequence_number': 1, 'section_marker': 'ZGS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2622</td>\n",
       "      <td>2622</td>\n",
       "      <td>[{'sequence_number': 1, 'section_marker': 'ZGS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2624</td>\n",
       "      <td>2624</td>\n",
       "      <td>[{'sequence_number': 1, 'section_marker': 'ZGS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  route                               section_requirements\n",
       "0  2408   2408  [{'sequence_number': 1, 'section_marker': 'ZGP...\n",
       "1   856    856  [{'sequence_number': 1, 'section_marker': 'ZGP...\n",
       "2  2620   2620  [{'sequence_number': 1, 'section_marker': 'ZGS...\n",
       "3  2622   2622  [{'sequence_number': 1, 'section_marker': 'ZGS...\n",
       "4  2624   2624  [{'sequence_number': 1, 'section_marker': 'ZGS..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_intentions = pd.DataFrame.from_dict(scenario['service_intentions'])\n",
    "service_intentions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence_number': 1,\n",
       "  'section_marker': 'ZGPP',\n",
       "  'type': 'start',\n",
       "  'entry_earliest': '06:20:00',\n",
       "  'entry_delay_weight': 1,\n",
       "  'exit_delay_weight': 1,\n",
       "  'connections': None},\n",
       " {'sequence_number': 2,\n",
       "  'section_marker': 'ZG_Halt',\n",
       "  'type': 'halt',\n",
       "  'min_stopping_time': 'PT48S',\n",
       "  'entry_latest': '06:27:00',\n",
       "  'entry_delay_weight': 1,\n",
       "  'exit_earliest': '06:29:00',\n",
       "  'connections': None},\n",
       " {'sequence_number': 3,\n",
       "  'section_marker': 'ZUE_Halt',\n",
       "  'type': 'halt',\n",
       "  'min_stopping_time': 'PT24S',\n",
       "  'entry_latest': '06:51:00',\n",
       "  'entry_delay_weight': 1,\n",
       "  'exit_latest': '06:56:00',\n",
       "  'exit_delay_weight': 1,\n",
       "  'connections': None}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "service_intentions.loc[0, ['section_requirements']][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now build the graph. Nodes are called \"previous_FAB -> next_FAB\" within lineare abschnittsfolgen and \"AK\" if\n",
    "# there is an Abschnittskennzeichen 'AK' on it\n",
    "route_graphs = dict()\n",
    "for route in scenario[\"routes\"]:\n",
    "    \n",
    "#     print(f\"\\nConstructing route graph for route {route['id']}\")\n",
    "    # set global graph settings\n",
    "    G = nx.DiGraph(route_id = route[\"id\"], name=\"Route-Graph for route \"+str(route[\"id\"]))\n",
    "\n",
    "    # add edges with data contained in the preprocessed graph\n",
    "    for path in route[\"route_paths\"]:\n",
    "        for (i, route_section) in enumerate(path[\"route_sections\"]):\n",
    "            route_section['route_path'] = path['id']\n",
    "            \n",
    "#             print(\"Adding Edge from {} to {} with sequence number {}\".format(from_node_id(path, route_section, i), to_node_id(path, route_section, i), route_section))\n",
    "            G.add_edge(from_node_id(path, route_section, i),\n",
    "                       to_node_id(path, route_section, i),\n",
    "                       data=route_section)\n",
    "\n",
    "    route_graphs[route[\"id\"]] = G\n",
    "\n",
    "# print(\"Finished building fahrweg-graphen in {} seconds\".format(str(time.time() - start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to visualize the graph. Plotting directly from networkx is unfortunately not be as easy to unterstand. This is the reason for outputing graphml files which will allow you to visualize the graph in a tool of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# route_graph = next(iter(route_graphs.values()))\n",
    "\n",
    "# from nxpd import draw\n",
    "# draw(route_graph, show='ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_requirements_for_path(path, train, train_id):\n",
    "    try:\n",
    "        path['section_marker'] = path[path['section_marker'].notnull()]['section_marker'].apply(lambda x: x[0])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    path = path.drop(['starting_point', 'ending_point'], axis=1)\n",
    "    \n",
    "    try:\n",
    "        path = path.drop(['route_alternative_marker_at_entry'], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        path = path.drop(['route_alternative_marker_at_exit'], axis=1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for moment in ['entry_earliest', 'entry_latest', 'exit_earliest', 'exit_latest', 'min_stopping_time']:\n",
    "        path[moment] = np.nan\n",
    "\n",
    "    for requirement in train:\n",
    "        for moment in ['entry_earliest', 'entry_latest', 'exit_earliest', 'exit_latest', 'min_stopping_time']:\n",
    "            if moment in requirement.keys():\n",
    "                path.loc[path['section_marker'] == requirement['section_marker'], moment] = requirement[moment]\n",
    "\n",
    "        for item in ['entry_delay_weight', 'exit_delay_weight', 'connections']:\n",
    "            try:\n",
    "                path.loc[path['section_marker'] == requirement['section_marker'], item] = requirement[item]\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    path['train_id'] = train_id\n",
    "         \n",
    "    return path\n",
    "\n",
    "# edges = add_requirements_for_path(edges, train)\n",
    "# edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_initial_entry_exit(path):\n",
    "    path['entry_time'] = path['entry_earliest'].iloc[0]\n",
    "\n",
    "    exit = [path['entry_time'].values[0]]\n",
    "    for i in range(0, len(path.index)):\n",
    "        proposition = exit[i] + path['minimum_running_time'].iloc[i] + path['min_stopping_time'].iloc[i]\n",
    "        try:\n",
    "            if proposition < path['exit_earliest'].iloc[i]:\n",
    "                exit.append(path['exit_earliest'].iloc[i])\n",
    "            else:\n",
    "                exit.append(proposition)\n",
    "        except:\n",
    "            exit.append(proposition)\n",
    "            \n",
    "    path['exit_time'] = exit[1:]\n",
    "    path.loc[1:, 'entry_time'] = path['exit_time'].shift(1)\n",
    "    \n",
    "    return path\n",
    "\n",
    "# edges = calculate_initial_entry_exit(edges)\n",
    "# edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(path):\n",
    "    path['entry_earliest'] = path['entry_earliest'].fillna(0)\n",
    "    path['entry_earliest'] = pd.to_timedelta(path['entry_earliest'], unit='s')\n",
    "    path['entry_latest'] = pd.to_timedelta(path['entry_latest'], unit='s')\n",
    "    path['exit_earliest'] = pd.to_timedelta(path['exit_earliest'], unit='s')\n",
    "    path['exit_latest'] = pd.to_timedelta(path['exit_latest'], unit='s')\n",
    "    path['minimum_running_time'] = pd.to_timedelta(path['minimum_running_time'].apply(lambda x: str(x).split('PT')[-1]), unit='s')\n",
    "    path['min_stopping_time'] = path['min_stopping_time'].fillna(0)\n",
    "    path['min_stopping_time'] = pd.to_timedelta(path['min_stopping_time'].apply(lambda x: str(x).split('PT')[-1]), unit='s')\n",
    "    \n",
    "    return path\n",
    "\n",
    "# edges = preprocess_train(edges)\n",
    "# edges\n",
    "\n",
    "def calculate_possible_train_paths(G, train, route_id, train_id):\n",
    "    train_paths = dict()\n",
    "    \n",
    "    for nodes in root_to_leaf_paths(G):\n",
    "        path = pd.DataFrame()\n",
    "\n",
    "        for n1, n2 in zip(nodes[:-1], nodes[1:]):\n",
    "            data = pd.DataFrame.from_dict(G.get_edge_data(n1,n2))\n",
    "            path = path.append(data['data'])\n",
    "        path = path.reset_index(drop=True)\n",
    "        \n",
    "        path = add_requirements_for_path(path, train, train_id)\n",
    "        path = preprocess_train(path)\n",
    "        path = calculate_initial_entry_exit(path)\n",
    "\n",
    "        train_paths[route_id] = path\n",
    "    return train_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains_with_paths = dict()\n",
    "\n",
    "for i, service_intention in service_intentions.iterrows():\n",
    "    train = service_intention['section_requirements']\n",
    "    train_id = service_intention['id']\n",
    "    route_id = service_intention['route']\n",
    "    route_graph = route_graphs[route_id]\n",
    "    \n",
    "    train_paths = calculate_possible_train_paths(route_graph, train, route_id, train_id)\n",
    "    trains_with_paths[train_id] = train_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 days 06:20:00\n",
      "0 days 07:20:00\n",
      "0 days 06:25:00\n",
      "0 days 06:52:00\n",
      "0 days 07:25:00\n",
      "0 days 07:52:00\n",
      "0 days 06:04:00\n",
      "0 days 07:04:00\n",
      "0 days 08:04:00\n",
      "0 days 08:18:00\n",
      "0 days 07:18:00\n",
      "0 days 06:34:00\n",
      "0 days 08:34:00\n",
      "0 days 07:02:00\n",
      "0 days 08:02:00\n",
      "0 days 06:07:00\n",
      "0 days 07:07:00\n",
      "0 days 06:31:00\n",
      "0 days 07:00:00\n",
      "0 days 07:31:00\n",
      "0 days 08:00:00\n",
      "0 days 08:31:00\n",
      "0 days 09:00:00\n",
      "0 days 07:32:00\n",
      "0 days 08:32:00\n",
      "0 days 08:35:00\n",
      "0 days 06:45:00\n",
      "0 days 07:15:00\n",
      "0 days 06:14:00\n",
      "0 days 06:44:00\n",
      "0 days 06:35:00\n",
      "0 days 07:05:00\n",
      "0 days 06:35:00\n",
      "0 days 07:05:00\n",
      "0 days 06:48:00\n",
      "0 days 07:18:00\n",
      "0 days 06:30:00\n",
      "0 days 07:00:00\n",
      "0 days 06:35:00\n",
      "0 days 07:35:00\n",
      "0 days 00:00:00\n",
      "0 days 00:00:00\n",
      "0 days 06:23:00\n",
      "0 days 06:49:00\n",
      "0 days 06:47:00\n",
      "0 days 07:17:00\n",
      "0 days 06:04:00\n",
      "0 days 06:31:00\n",
      "0 days 06:20:00\n",
      "0 days 06:38:00\n",
      "0 days 06:13:00\n",
      "0 days 06:06:00\n",
      "0 days 06:43:00\n",
      "0 days 06:36:00\n",
      "0 days 06:13:00\n",
      "0 days 07:13:00\n",
      "0 days 06:44:00\n",
      "0 days 07:44:00\n"
     ]
    }
   ],
   "source": [
    "chosen_paths = dict()\n",
    "for train in trains_with_paths.keys():\n",
    "    chosen_paths[train] = next(iter(trains_with_paths[train].keys()))\n",
    "\n",
    "for train, path in chosen_paths.items():\n",
    "    print(trains_with_paths[train][path].loc[0,'entry_earliest'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_used_resources(used_resources, path):\n",
    "    for i, row in path.iterrows():\n",
    "#         print(row['resource_occupations'])\n",
    "        for j in row['resource_occupations']:\n",
    "            if j['resource'] not in used_resources.keys():\n",
    "                used_resources[j['resource']] = pd.DataFrame()\n",
    "            used_resources[j['resource']] = used_resources[j['resource']].append(path.loc[i, ['train_id', 'entry_time', 'exit_time']])\n",
    "    #         used_resources[j['resource']] = used_resources[j['resource']].reset_index(drop=True)\n",
    "    return used_resources\n",
    "\n",
    "def group_trains(used_resources):\n",
    "    for resource in used_resources.keys():\n",
    "        used_resources[resource] = used_resources[resource].groupby('train_id').agg({'entry_time': np.min, 'exit_time': np.max})\n",
    "    return used_resources\n",
    "\n",
    "def add_release_time(used_resources):\n",
    "    for key in used_resources.keys():\n",
    "        used_resources[key]['exit_time'] += resources[resources['id'] == key]['release_time'].values[0]\n",
    "    return used_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_resources = dict() \n",
    "\n",
    "for train_id in trains_with_paths.keys():\n",
    "    route_id = chosen_paths[train_id]\n",
    "    path = trains_with_paths[train_id][route_id]\n",
    "    used_resources = gather_used_resources(used_resources, path)\n",
    "\n",
    "# for path in chosen_paths:\n",
    "#     used_resources = gather_used_resources(used_resources, path)\n",
    "    \n",
    "    \n",
    "used_resources = group_trains(used_resources)\n",
    "used_resources = add_release_time(used_resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "8\n",
      "6\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "10\n",
      "10\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "21\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "6\n",
      "21\n",
      "21\n",
      "9\n",
      "9\n",
      "9\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "12\n",
      "10\n",
      "10\n",
      "10\n",
      "5\n",
      "5\n",
      "9\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "16\n",
      "16\n",
      "16\n",
      "9\n",
      "9\n",
      "9\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "11\n",
      "7\n",
      "7\n",
      "15\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "13\n",
      "11\n",
      "11\n",
      "15\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "2\n",
      "8\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "21\n",
      "21\n",
      "21\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "15\n",
      "13\n",
      "13\n",
      "13\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "15\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "9\n",
      "9\n",
      "13\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "7\n",
      "7\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "2\n",
      "2\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "6\n",
      "6\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "6\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "10\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "2\n",
      "4\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "6\n",
      "6\n",
      "2\n",
      "2\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "4\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for key in used_resources.keys():\n",
    "    print(len(used_resources[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_resources_collisions_in_seconds(used_resources):\n",
    "    s = 0\n",
    "    for resource in used_resources.keys():\n",
    "        used_resources[resource] = used_resources[resource].sort_values(by=['entry_time'])\n",
    "        used_resources[resource]['collision'] = (used_resources[resource]['exit_time'] - used_resources[resource]['entry_time'].shift(-1)).dt.total_seconds()\n",
    "        used_resources[resource].loc[used_resources[resource]['collision'] < 0, 'collision'] = 0\n",
    "        s += used_resources[resource]['collision'].sum()\n",
    "    return s\n",
    "       \n",
    "calculate_resources_collisions_in_seconds(used_resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_earliest_violation(chosen_trains, event='entry', time='earliest'):\n",
    "    s = 0\n",
    "    for train in chosen_trains:\n",
    "        requirements = train[train[event + '_' + time].notnull()].copy()\n",
    "        requirements['violation'] = (requirements[event + '_' + time].astype('timedelta64[s]') - requirements[event + '_time'])\n",
    "        if time == 'earliest':\n",
    "            requirements.loc[requirements['violation'] < 0, 'violation'] = 0\n",
    "        else:\n",
    "            requirements.loc[requirements['violation'] > 0, 'violation'] = 0\n",
    "            requirements['violation'] = -requirements['violation']\n",
    "        s += requirements['violation'].sum()\n",
    "    return s\n",
    "\n",
    "print(calculate_earliest_violation(chosen_trains, 'entry', 'earliest'))\n",
    "print(calculate_earliest_violation(chosen_trains, 'exit', 'earliest'))\n",
    "print(calculate_earliest_violation(chosen_trains, 'entry', 'latest'))\n",
    "print(calculate_earliest_violation(chosen_trains, 'exit', 'latest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_running_time_violation(chosen_trains):\n",
    "    s = 0\n",
    "    for train in chosen_trains:\n",
    "        train['violation'] = (train['minimum_running_time'] - (train['exit_time'] - train['entry_time'])).dt.total_seconds()\n",
    "        train.loc[train['violation'] < 0, 'violation'] = 0\n",
    "        s += train['violation'].sum()\n",
    "    return s\n",
    "\n",
    "calculate_running_time_violation(chosen_trains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timetables = []\n",
    "for train in chosen_trains:\n",
    "    timetable = np.append(train['entry_time'].values[0], train['exit_time'].values).astype('timedelta64[s]').astype('float64')\n",
    "#     print(np.append(train['entry_time'].values[0], train['exit_time'].values))\n",
    "    timetables.append(timetable)\n",
    " \n",
    "print(timetables)\n",
    "chain =  np.append(chosen_trains[0]['entry_time'].values[0], chosen_trains[0]['exit_time'].values).astype('timedelta64[s]').astype('float64')\n",
    "chain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gentic Algorythm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy\n",
    "\n",
    "from deap import algorithms\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "\n",
    "from itertools import repeat\n",
    "from collections import Sequence\n",
    "import math\n",
    "import time\n",
    "\n",
    "creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0, -0.5))\n",
    "creator.create(\"Individual\", numpy.ndarray, fitness=creator.FitnessMin)\n",
    "\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "def give_chain():\n",
    "#     return chain.astype(np.float64) + 1e11*np.random.randn(len(chain)).astype(np.float64)\n",
    "    return timetables\n",
    "\n",
    "\n",
    "def evalOneMax(individual):\n",
    "    time1 = time.time()\n",
    "    fitness = []\n",
    "    for i in individual:\n",
    "#         used_resources = dict() \n",
    "#         for train in range(len(i)):\n",
    "#             used_resources = gather_used_resources(used_resources, chosen_trains[train])\n",
    "#         used_resources = group_trains(used_resources)\n",
    "#         used_resources = add_release_time(used_resources)\n",
    "        for train in range(len(i)):\n",
    "            chosen_trains[train]['entry_time'] = i[train][:-1].astype('timedelta64[s]')\n",
    "            chosen_trains[train]['exit_time'] = i[train][1:].astype('timedelta64[s]')\n",
    "\n",
    "            time1 = time.time()\n",
    "            \n",
    "            m = 0\n",
    "            d = 0\n",
    "                       \n",
    "#             m += calculate_resources_collisions_in_seconds(used_resources)           \n",
    "            m += calculate_earliest_violation(chosen_trains, 'entry', 'earliest')\n",
    "            m += calculate_earliest_violation(chosen_trains, 'exit', 'earliest')\n",
    "            d += calculate_earliest_violation(chosen_trains, 'entry', 'latest')\n",
    "            d += calculate_earliest_violation(chosen_trains, 'exit', 'latest')\n",
    "            m += calculate_running_time_violation(chosen_trains)\n",
    "            time2 = time.time()\n",
    "#             print('{:s} function took {:.3f} ms'.format('collisions', (time2-time1)*1000.0))\n",
    "\n",
    "        fitness.append(m+d)\n",
    "    time2 = time.time()\n",
    "    \n",
    "    return fitness\n",
    "\n",
    "toolbox.register(\"attr_bool\", give_chain)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=1)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "def cxTwoPointCopy(ind1, ind2):\n",
    "#     \"\"\"Execute a two points crossover with copy on the input individuals. The\n",
    "#     copy is required because the slicing in numpy returns a view of the data,\n",
    "#     which leads to a self overwritting in the swap operation. It prevents\n",
    "#     ::\n",
    "    \n",
    "#         >>> import numpy\n",
    "#         >>> a = numpy.array((1,2,3,4))\n",
    "#         >>> b = numpy.array((5.6.7.8))\n",
    "#         >>> a[1:3], b[1:3] = b[1:3], a[1:3]\n",
    "#         >>> print(a)\n",
    "#         [1 6 7 4]\n",
    "#         >>> print(b)\n",
    "#         [5 6 7 8]\n",
    "#     \"\"\"\n",
    "# #     time1 = time.time()\n",
    "#     for i in range(len(ind1)):\n",
    "#         size = len(ind1[0][i])\n",
    "#         cxpoint1 = random.randint(1, size)\n",
    "#         cxpoint2 = random.randint(1, size - 1)\n",
    "#         if cxpoint2 >= cxpoint1:\n",
    "#             cxpoint2 += 1\n",
    "#         else: # Swap the two cx points\n",
    "#             cxpoint1, cxpoint2 = cxpoint2, cxpoint1\n",
    "\n",
    "#         ind1[0][i][cxpoint1:cxpoint2], ind2[0][i][cxpoint1:cxpoint2] \\\n",
    "#             = ind2[0][i][cxpoint1:cxpoint2].copy(), ind1[0][i][cxpoint1:cxpoint2].copy()\n",
    "# #     time2 = time.time()\n",
    "# #     print('{:s} function took {:.3f} ms'.format('cxTwoPointCopy', (time2-time1)*1000.0))\n",
    "    return ind1, ind2\n",
    "\n",
    "def mutGaussian(individual, mu, sigma, indpb, shiftpb):\n",
    "#     time1 = time.time()\n",
    "    \"\"\"This function applies a gaussian mutation of mean *mu* and standard\n",
    "    deviation *sigma* on the input individual. This mutation expects a\n",
    "    :term:`sequence` individual composed of real valued attributes.\n",
    "    The *indpb* argument is the probability of each attribute to be mutated.\n",
    "    :param individual: Individual to be mutated.\n",
    "    :param mu: Mean or :term:`python:sequence` of means for the\n",
    "               gaussian addition mutation.\n",
    "    :param sigma: Standard deviation or :term:`python:sequence` of\n",
    "                  standard deviations for the gaussian addition mutation.\n",
    "    :param indpb: Independent probability for each attribute to be mutated.\n",
    "    :returns: A tuple of one individual.\n",
    "    This function uses the :func:`~random.random` and :func:`~random.gauss`\n",
    "    functions from the python base :mod:`random` module.\n",
    "    \"\"\"\n",
    "    size = len(individual)\n",
    "#     print(individual)\n",
    "    if not isinstance(mu, Sequence):\n",
    "        mu = repeat(mu, size)\n",
    "    elif len(mu) < size:\n",
    "        raise IndexError(\"mu must be at least the size of individual: %d < %d\" % (len(mu), size))\n",
    "    if not isinstance(sigma, Sequence):\n",
    "        sigma = repeat(sigma, size)\n",
    "    elif len(sigma) < size:\n",
    "        raise IndexError(\"sigma must be at least the size of individual: %d < %d\" % (len(sigma), size))\n",
    "\n",
    "    if random.random() < shiftpb:\n",
    "        for i, m, s in zip(range(size), mu, sigma):      \n",
    "            individual[i] += random.gauss(m, s)\n",
    "    for i, m, s in zip(range(size), mu, sigma):      \n",
    "        if random.random() < indpb:\n",
    "            individual[i] += random.gauss(m, s)\n",
    "#     time2 = time.time()\n",
    "#     print('{:s} function took {:.3f} ms'.format('mutGaussian', (time2-time1)*1000.0))\n",
    "    return individual,\n",
    "    \n",
    "import multiprocessing\n",
    "\n",
    "pool = multiprocessing.Pool()\n",
    "toolbox.register(\"map\", pool.map)\n",
    "\n",
    "toolbox.register(\"evaluate\", evalOneMax)\n",
    "toolbox.register(\"mate\", cxTwoPointCopy)\n",
    "toolbox.register(\"mutate\", mutGaussian, mu=0, sigma=1e2, indpb=0.01, shiftpb=0.4)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%prun -s cumulative -q -l 100 -T prun0\n",
    "\n",
    "def main():\n",
    "    random.seed(64)\n",
    "    \n",
    "    pop = toolbox.population(n=40)\n",
    "    \n",
    "    # Numpy equality function (operators.eq) between two arrays returns the\n",
    "    # equality element wise, which raises an exception in the if similar()\n",
    "    # check of the hall of fame. Using a different equality function like\n",
    "    # numpy.array_equal or numpy.allclose solve this issue.\n",
    "    hof = tools.HallOfFame(1, similar=numpy.array_equal)\n",
    "    \n",
    "    stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "    stats.register(\"avg\", numpy.mean)\n",
    "    stats.register(\"std\", numpy.std)\n",
    "    stats.register(\"min\", numpy.min)\n",
    "#     stats.register(\"max\", numpy.max)\n",
    "    \n",
    "    algorithms.eaSimple(pop, toolbox, cxpb=0.4, mutpb=0.2, ngen=100, stats=stats,\n",
    "                        halloffame=hof)\n",
    "\n",
    "    return pop, stats, hof\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pop, stats, hof = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = hof[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_trains[0]['entry_time'] = a[0][:-1].astype('timedelta64[s]')\n",
    "chosen_trains[0]['exit_time'] = a[0][1:].astype('timedelta64[s]')\n",
    "chosen_trains[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['violation'] = (train['minimum_running_time'] - train['exit_time'] - train['entry_time']).dt.total_seconds()\n",
    "\n",
    "# 8:30 8:32 \n",
    "# entry exit\n",
    "# exit - entry = 2\n",
    "\n",
    "# 4\n",
    "# minimum\n",
    "# minimum - (exit - entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(open('prun0', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
